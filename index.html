<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>InfiniteDance: Scalable 3D Dance Generation</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', sans-serif;
            background-color: #f5f5f5;
            color: #333;
            line-height: 1.8;
            padding-bottom: 40px;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
        }

        /* Header Section */
        header {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 60px 20px 80px;
            text-align: center;
        }

        header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            margin-bottom: 20px;
            line-height: 1.2;
        }

        .authors {
            font-size: 1.1rem;
            margin-top: 20px;
            opacity: 0.95;
        }

        .affiliations {
            font-size: 0.95rem;
            margin-top: 10px;
            opacity: 0.9;
        }

        .links {
            margin-top: 25px;
        }

        .links a {
            color: white;
            text-decoration: none;
            margin: 0 15px;
            font-size: 1rem;
            padding: 8px 20px;
            border: 2px solid white;
            border-radius: 5px;
            display: inline-block;
            transition: all 0.3s;
        }

        .links a:hover {
            background: white;
            color: #2a5298;
        }

        /* Main Content */
        .content {
            background: white;
            margin-top: -50px;
            border-radius: 10px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
            padding: 50px 60px;
            position: relative;
            z-index: 10;
        }

        section {
            margin-bottom: 50px;
        }

        section:last-child {
            margin-bottom: 0;
        }

        h2 {
            font-size: 2rem;
            font-weight: 600;
            margin-bottom: 25px;
            color: #1e3c72;
            border-bottom: 3px solid #2a5298;
            padding-bottom: 10px;
        }

        h3 {
            font-size: 1.4rem;
            font-weight: 600;
            margin: 30px 0 15px;
            color: #2a5298;
        }

        /* Abstract */
        .abstract {
            font-size: 1.05rem;
            line-height: 1.9;
            text-align: justify;
        }

        .abstract strong {
            color: #1e3c72;
            font-weight: 600;
        }

        /* Method Section */
        .method-figure {
            width: 100%;
            margin: 30px 0;
            text-align: center;
        }

        .method-figure img,
        .method-figure object,
        .method-figure embed {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        .method-figure object,
        .method-figure embed {
            display: block;
            margin: 0 auto;
        }

        .method-figure figcaption {
            margin-top: 15px;
            font-size: 0.95rem;
            color: #666;
            font-style: italic;
        }

        .method-text {
            font-size: 1.05rem;
            line-height: 1.9;
            margin-top: 20px;
        }

        /* Teaser */
        .teaser-figure {
            width: 100%;
            margin: 30px 0;
            text-align: center;
        }

        .teaser-figure img,
        .teaser-figure object,
        .teaser-figure embed {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }

        .teaser-figure object,
        .teaser-figure embed {
            display: block;
            margin: 0 auto;
        }

        /* Video Section */
        .video-wrapper {
            width: 100%;
            margin: 30px 0;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            background: #000;
        }

        video {
            width: 100%;
            height: auto;
            display: block;
        }

        /* BibTeX */
        .bibtex {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #2a5298;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        /* Footer */
        footer {
            text-align: center;
            margin-top: 50px;
            padding: 30px 20px;
            color: #888;
            font-size: 0.9rem;
        }

        /* Responsive */
        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8rem;
            }

            .content {
                padding: 30px 25px;
                margin-top: -40px;
            }

            h2 {
                font-size: 1.6rem;
            }

            .abstract, .method-text {
                font-size: 1rem;
            }

            .links a {
                display: block;
                margin: 10px 0;
            }
        }
    </style>
</head>
<body>

    <header>
        <div class="container">
            <h1>InfiniteDance: Scalable 3D Dance Generation Towards in-the-wild Generalization</h1>
            <div class="authors">
                <!-- Add authors here if available -->
            </div>
            <div class="affiliations">
                <!-- Add affiliations here if available -->
            </div>
            <div class="links">
                <a href="#">arXiv</a>
                <a href="#">Code</a>
            </div>
        </div>
    </header>

    <div class="container">
        <div class="content">
            
            <!-- Abstract Section -->
            <section id="abstract">
                <h2>Abstract</h2>
                <div class="abstract">
                    Although existing 3D dance generation methods perform well in controlled scenarios, they often struggle to generalize in the wild. When conditioned on unseen music, existing methods often produce unstructured or physically implausible dance, largely due to limited music-to-dance data and restricted model capacity. This work aims to push the frontier of generalizable 3D dance generation by scaling up both data and model design. <strong>1)</strong> On the data side, we develop a fully automated pipeline that reconstructs high-fidelity 3D dance motions from monocular videos. To eliminate the physical artifacts prevalent in existing reconstruction methods, we introduce a Foot Restoration Diffusion Model (FRDM) guided by foot-contact and geometric constraints that enforce physical plausibility while preserving kinematic smoothness and expressiveness, resulting in a diverse, high-quality multimodal 3D dance dataset totaling [OurHours] hours. <strong>2)</strong> On model design, we propose Choreographic LLaMA (ChoreoLLaMA), a scalable LLaMA-based architecture. To enhance robustness under unfamiliar music conditions, we integrate a retrieval-augmented generation (RAG) module that injects reference dance as a prompt. Additionally, we design a slow/fast-cadence Mixture-of-Experts (MoE) module that enables ChoreoLLaMA to smoothly adapt motion rhythms across varying music tempos. Extensive experiments across diverse dance genres show that our approach surpasses existing methods in both qualitative and quantitative evaluations, marking a step toward scalable, real-world 3D dance generation.
                </div>
            </section>

            <!-- Teaser Section -->
            <section id="teaser">
                <h2>Teaser</h2>
                <figure class="teaser-figure">
                    <img src="images/teaser3.png" alt="Teaser" style="max-width: 100%; height: auto; border-radius: 8px;">
                </figure>
            </section>

            <!-- Method Section -->
            <section id="method">
                <h2>Method</h2>
                <figure class="method-figure">
                    <img src="images/ChoreoLLaMA.png" alt="ChoreoLLaMA Overview" style="max-width: 100%; height: auto; border-radius: 8px;">
                    <figcaption>Fig. 1 The overview of ChoreoLLaMA. Our approach consists of two main components: (1) a fully automated pipeline for high-fidelity 3D dance reconstruction with Foot Restoration Diffusion Model (FRDM), and (2) Choreographic LLaMA (ChoreoLLaMA) architecture with Retrieval-Augmented Generation (RAG) and slow/fast-cadence Mixture-of-Experts (MoE) modules for robust dance generation.</figcaption>
                </figure>
                <div class="method-text">
                    <p>In order to achieve scalable and generalizable 3D dance generation, we design a two-pronged approach that addresses both data quality and model architecture.</p>
                    
                    <h3>Data Pipeline with Foot Restoration</h3>
                    <p>We develop a fully automated pipeline that reconstructs high-fidelity 3D dance motions from monocular videos. To eliminate physical artifacts prevalent in existing reconstruction methods, we introduce a Foot Restoration Diffusion Model (FRDM) guided by foot-contact and geometric constraints. This ensures physical plausibility while preserving kinematic smoothness and expressiveness, resulting in a diverse, high-quality multimodal 3D dance dataset.</p>
                    
                    <h3>Choreographic LLaMA Architecture</h3>
                    <p>We propose Choreographic LLaMA (ChoreoLLaMA), a scalable LLaMA-based architecture for 3D dance generation. To enhance robustness under unfamiliar music conditions, we integrate a retrieval-augmented generation (RAG) module that injects reference dance as a prompt. Additionally, we design a slow/fast-cadence Mixture-of-Experts (MoE) module that enables ChoreoLLaMA to smoothly adapt motion rhythms across varying music tempos.</p>
                </div>
            </section>

            <!-- Results Section -->
            <section id="results">
                <h2>Results</h2>
                <p style="margin-bottom: 20px; font-size: 1.05rem;">As shown in the following videos (Please unmute for music), InfiniteDance can generate high-quality 3D dances from given music with improved generalization capabilities.</p>
                
                <div class="video-wrapper">
                    <video controls autoplay loop muted playsinline>
                        <source src="videos/Infinitedemo.mp4" type="video/mp4">
                    </video>
                </div>
            </section>



        </div>
    </div>

    <footer>
        <p>&copy; 2026 InfiniteDance Project. Template based on <a href="https://github.com/lioryariv/idr" style="color: #2a5298;">Lodge project page</a>.</p>
    </footer>

</body>
</html>
